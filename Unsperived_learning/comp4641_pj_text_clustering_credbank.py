# -*- coding: utf-8 -*-
"""COMP4641_PJ_TEXT_CLUSTERING_CREDBANK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ryg8MAor1XoIpR_Pm8sKvbXSI8XfIV7L
"""

import numpy as np
import pandas as pd
import gensim
import time
import statistics as st

from gensim import corpora
from pprint import pprint
from gensim.models import Word2Vec
from collections import Counter

import numpy as np 
import nltk 
from gensim.models import Word2Vec
from nltk.cluster import KMeansClusterer    
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer 
from nltk.tokenize import TweetTokenizer
from nltk.stem import PorterStemmer 

from sklearn import cluster
from sklearn import metrics
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from matplotlib import pyplot as plt

import re
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

full_dataset = pd.read_csv("part4_data/Part4_Result_1000v.csv",error_bad_lines=False,engine='python' )


#########################################################################################################
#The Elbow Method
def calculate_WSS(points, kmax): 
  sse = []
  for k in range(1, kmax+1):
    kmeans = KMeans(n_clusters = k).fit(points)
    centroids = kmeans.cluster_centers_
    pred_clusters = kmeans.predict(points)
    curr_sse = 0
    
    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS
    for i in range(len(points)):
      curr_center = centroids[pred_clusters[i]]
      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2
      
    sse.append(curr_sse)
    plt.plot(sse)
  return sse

#########################################################################################################
#vectorization of sentence 
def sent_vectorizer(sent):
    sent_vec =[]
    numw = 0
    for w in sent:
        try:
            sent_vec.append(model[w])
        except:
            pass
     
    return sent_vec


#########################################################################################################
#lemmatization od sentence 
def lemmatization(text):
    return [lemmatizer.lemmatize(w) for w in text]
#########################################################################################################
#stemming word  
def stemmeriation(text):
    return [ps.stem(w) for w in text]
    
#########################################################################################################
#get the mean of the vector of a sentence 
def avg_vect(sent_vec):
    sum_of_vec = 0
    for word in sent_vec:
      sum_of_vec+= np.mean(word)
    if(len(sent_vec)>0):
      return (sum_of_vec/len(sent_vec))  
    return 0
#########################################################################################################
#print each sentence 
def sent_print(sent):
    print(sent)

full_dataset.columns = ['index','text','id','author_id','create_date','in_reply_to_status_id_str','in_reply_to_user_id_str','in_reply_to_screen_name','quoted_status_id_str','retweet_id','retweet_author_id','event']
print(full_dataset.loc[0])

#text pre-processing
not_words = ['!','@','?',',','.',"'", ')',';','(', '/',':','-','&','*','"','\'','\"','...','..','=]','=[','rt']

tweet_dataset = full_dataset[['id','text']]
print(tweet_dataset.loc[0])
print(tweet_dataset.loc[1])
print(tweet_dataset.loc[2])
tweet_tokenizer = TweetTokenizer()
tweet_tokens = []
for sent in tweet_dataset['text']:
    tweet_tokens.append(tweet_tokenizer.tokenize(str(sent).lower()))
tweet_dataset['tokenized_text'] = tweet_tokens

tweet_dataset['tokenized_text'] = tweet_dataset['tokenized_text'].apply(lambda x: [item for item in x if item not in not_words])

for sentence in tweet_dataset['tokenized_text']: 
    num_to_del = 0
    for word in sentence:
        if '@' in word: 
            num_to_del +=1
    for i in range(0,num_to_del):
        for word in sentence:
            if '@' in word: 
                sentence.remove(word)  
                
for sentence in tweet_dataset['tokenized_text']: 
    num_to_del = 0
    for word in sentence:
        if 'http' in word: 
            num_to_del +=1
    for i in range(0,num_to_del):
        for word in sentence:
            if 'http' in word: 
                sentence.remove(word)

#remove stop words
stop_words = set(stopwords.words('english')) 
tweet_dataset['text_without_stop_word'] = tweet_dataset['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])
#stem
ps = PorterStemmer() 
tweet_dataset['stemmed_text'] = tweet_dataset['text_without_stop_word'].apply(stemmeriation)

#lemma
lemmatizer = WordNetLemmatizer() 
tweet_dataset['lemmatized_text'] = tweet_dataset['stemmed_text'].apply(lemmatization)

print(tweet_dataset['lemmatized_text'])

#build word2vec model and run on each sentences 
model = Word2Vec(tweet_dataset['lemmatized_text'], min_count=1,size= 32) 
#tweet_dataset['vector_text'] = np.array([model[word] for word in (model.wv.vocab)])
tweet_dataset['vector_text'] = tweet_dataset['lemmatized_text'].apply(sent_vectorizer)
print((tweet_dataset['vector_text'][0]))

############################################################################################
#get the avg vector of all vectors in a sentence 
def avg_vector(vect_list):
  return np.mean(vect_list, axis=0)

tweet_dataset['avg_vector_text'] = tweet_dataset['vector_text'].apply(avg_vector)

############################################################################################
#replace all nan value of sentence into zero vector
def get_list(vect_list):
  re_list = []
  #print(vect_list)
  if not np.isnan(vect_list).any():   
    return vect_list
  else:
    return ([0] * 32)
############################################################################################
#iterative to call the 
def loop_rolist(vect_list):
  return vect_list.tolist()
############################################################################################
#return the len of the sentence 
def get_text_len(text):
  return len(text)


tweet_dataset['avg_vector_text'] = tweet_dataset['avg_vector_text'].apply(get_list)
print(tweet_dataset['avg_vector_text'])
vec_data = pd.DataFrame(tweet_dataset['avg_vector_text'].tolist())

print(vec_data)

#vec_data: pandas to cluster, contain the 32 col of average vector and one col contain the length of a the sentence

#normalization
from sklearn import preprocessing

x = vec_data 
min_max_scaler = preprocessing.Normalizer()
normalizsed_dataset_array = min_max_scaler.fit_transform(x)
nor_df = pd.DataFrame(normalizsed_dataset_array)
plt.plot(nor_df[0:1000])


#k-mean clusteirng 
kmeans = cluster.KMeans(n_clusters=4)
start_time = time.time()
kmeans.fit(normalizsed_dataset_array)
print("--- %s seconds ---" % (time.time() - start_time))
labels = kmeans.labels_
print ("Cluster id labels for inputted data")
print (labels)

#make gp as a col and output it as a csv file
full_dataset['gp'] = labels 
full_dataset.to_csv('part4_data/k_mean_word2vec_CREDBANK_1000v_2805.csv',index=False)

tweet_dataset['gp'] = labels 
print(tweet_dataset.loc[0])

#Optional to run, try to print out the top n frequnecy words of a gp
from collections import Counter

top_word_list =[]
for i in range(9):
    list1 = []
    list2 = []
    list1=tweet_dataset.lemmatized_text[tweet_dataset['gp']==i]
    list2 = sum(list1, [])
    #print(list2)
    counts = Counter(list2).most_common(40)
    print(counts)
    top_word_list.append(counts)
print("end")
