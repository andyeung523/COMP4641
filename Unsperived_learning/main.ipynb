{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llL9YyNHg0HJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "BegXHRaxmfG6",
    "outputId": "9f3926b9-a498-45f9-b5fb-63072e315bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_key          topic_key\n",
      "topic_terms      topic_terms\n",
      "Cred_Ratings    Cred_Ratings\n",
      "Reasons              Reasons\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cred_df = pd.read_csv('cred_event_TurkRatings.data', sep=\"\\t\",header=None)\n",
    "random.seed(50)\n",
    "n = 8302219 #number of records in file\n",
    "s = 100000 #desired sample size\n",
    "skip = sorted(random.sample(range(n),n-s))\n",
    "data_df = pd.read_csv('Part4_Result.csv',engine='python',skiprows=skip)\n",
    "data_df.columns=['text', 'id', 'author_id', 'created_at',\n",
    "       'in_reply_to_status_id_str', 'in_reply_to_user_id_str',\n",
    "       'in_reply_to_screen_name', 'quoted_status_id_str', 'retweet_id',\n",
    "       'retweet_author_id', 'topic']\n",
    "cred_df.columns = ['topic_key','topic_terms','Cred_Ratings','Reasons']\n",
    "print(cred_df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gnALOJ__n5ps",
    "outputId": "3153e48f-3a7c-490e-b042-63a91df25317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                         RT @thefashionisto: Neil Patrick Harris Tapped...\n",
      "id                                                          522749079617806336\n",
      "author_id                                                           2253554083\n",
      "created_at                                                 2014-10-16 14:01:07\n",
      "in_reply_to_status_id_str                                                  NaN\n",
      "in_reply_to_user_id_str                                                    NaN\n",
      "in_reply_to_screen_name                                                    NaN\n",
      "quoted_status_id_str                                                      None\n",
      "retweet_id                                                  522748889204809728\n",
      "retweet_author_id                                                     15325349\n",
      "topic                                                        host,patrick,neil\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>retweet_author_id</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @thefashionisto: Neil Patrick Harris Tapped...</td>\n",
       "      <td>522749079617806336</td>\n",
       "      <td>2253554083</td>\n",
       "      <td>2014-10-16 14:01:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>522748889204809728</td>\n",
       "      <td>15325349</td>\n",
       "      <td>host,patrick,neil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neil Patrick Harris to host Oscars: Neil Patri...</td>\n",
       "      <td>522744418126675972</td>\n",
       "      <td>377665274</td>\n",
       "      <td>2014-10-16 13:42:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>host,patrick,neil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 2015 @TheAcademy host @ActuallyNPH on @Mat...</td>\n",
       "      <td>522744773879558145</td>\n",
       "      <td>36044387</td>\n",
       "      <td>2014-10-16 13:44:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>host,patrick,neil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @billboard: Oscar Host Neil Patrick Harris:...</td>\n",
       "      <td>522742777411407872</td>\n",
       "      <td>179568487</td>\n",
       "      <td>2014-10-16 13:36:04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>522716566610530304</td>\n",
       "      <td>9695312</td>\n",
       "      <td>host,patrick,neil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neil Patrick Harris set to host Oscars 2015 ht...</td>\n",
       "      <td>522741609969225728</td>\n",
       "      <td>2773297820</td>\n",
       "      <td>2014-10-16 13:31:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>host,patrick,neil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75341</th>\n",
       "      <td>RT @HelloImIvan: odell beckham jr could catch ...</td>\n",
       "      <td>536720198402060288</td>\n",
       "      <td>153282458</td>\n",
       "      <td>2014-11-24 03:17:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>536719881299697665</td>\n",
       "      <td>2379738985</td>\n",
       "      <td>music,beijing,wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75342</th>\n",
       "      <td>RT @ericsports: Zoomed in picture of Odell Bec...</td>\n",
       "      <td>536720169641705473</td>\n",
       "      <td>314385813</td>\n",
       "      <td>2014-11-24 03:17:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>536709870976659456</td>\n",
       "      <td>1067944501</td>\n",
       "      <td>music,beijing,wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75343</th>\n",
       "      <td>RT @HelloImIvan: odell beckham jr could catch ...</td>\n",
       "      <td>536720146581430273</td>\n",
       "      <td>297042888</td>\n",
       "      <td>2014-11-24 03:17:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>536719881299697665</td>\n",
       "      <td>2379738985</td>\n",
       "      <td>music,beijing,wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75344</th>\n",
       "      <td>RT @ShannonSharpe: I've been to 4 county fairs...</td>\n",
       "      <td>536720156417069057</td>\n",
       "      <td>99869923</td>\n",
       "      <td>2014-11-24 03:17:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>536717467662700544</td>\n",
       "      <td>371539066</td>\n",
       "      <td>music,beijing,wave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75345</th>\n",
       "      <td>â€œ@nfl: Odell Beckham just broke the internet...</td>\n",
       "      <td>536720064452780032</td>\n",
       "      <td>548837960</td>\n",
       "      <td>2014-11-24 03:16:49</td>\n",
       "      <td>5.367042e+17</td>\n",
       "      <td>19426551.0</td>\n",
       "      <td>NFL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>music,beijing,wave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75346 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text                  id  \\\n",
       "0      RT @thefashionisto: Neil Patrick Harris Tapped...  522749079617806336   \n",
       "1      Neil Patrick Harris to host Oscars: Neil Patri...  522744418126675972   \n",
       "2      The 2015 @TheAcademy host @ActuallyNPH on @Mat...  522744773879558145   \n",
       "3      RT @billboard: Oscar Host Neil Patrick Harris:...  522742777411407872   \n",
       "4      Neil Patrick Harris set to host Oscars 2015 ht...  522741609969225728   \n",
       "...                                                  ...                 ...   \n",
       "75341  RT @HelloImIvan: odell beckham jr could catch ...  536720198402060288   \n",
       "75342  RT @ericsports: Zoomed in picture of Odell Bec...  536720169641705473   \n",
       "75343  RT @HelloImIvan: odell beckham jr could catch ...  536720146581430273   \n",
       "75344  RT @ShannonSharpe: I've been to 4 county fairs...  536720156417069057   \n",
       "75345  â€œ@nfl: Odell Beckham just broke the internet...  536720064452780032   \n",
       "\n",
       "        author_id           created_at  in_reply_to_status_id_str  \\\n",
       "0      2253554083  2014-10-16 14:01:07                        NaN   \n",
       "1       377665274  2014-10-16 13:42:36                        NaN   \n",
       "2        36044387  2014-10-16 13:44:00                        NaN   \n",
       "3       179568487  2014-10-16 13:36:04                        NaN   \n",
       "4      2773297820  2014-10-16 13:31:26                        NaN   \n",
       "...           ...                  ...                        ...   \n",
       "75341   153282458  2014-11-24 03:17:21                        NaN   \n",
       "75342   314385813  2014-11-24 03:17:14                        NaN   \n",
       "75343   297042888  2014-11-24 03:17:09                        NaN   \n",
       "75344    99869923  2014-11-24 03:17:11                        NaN   \n",
       "75345   548837960  2014-11-24 03:16:49               5.367042e+17   \n",
       "\n",
       "       in_reply_to_user_id_str in_reply_to_screen_name quoted_status_id_str  \\\n",
       "0                          NaN                     NaN                 None   \n",
       "1                          NaN                     NaN                 None   \n",
       "2                          NaN                     NaN                 None   \n",
       "3                          NaN                     NaN                 None   \n",
       "4                          NaN                     NaN                 None   \n",
       "...                        ...                     ...                  ...   \n",
       "75341                      NaN                     NaN                 None   \n",
       "75342                      NaN                     NaN                 None   \n",
       "75343                      NaN                     NaN                 None   \n",
       "75344                      NaN                     NaN                 None   \n",
       "75345               19426551.0                     NFL                 None   \n",
       "\n",
       "               retweet_id retweet_author_id               topic  \n",
       "0      522748889204809728          15325349   host,patrick,neil  \n",
       "1                    None              None   host,patrick,neil  \n",
       "2                    None              None   host,patrick,neil  \n",
       "3      522716566610530304           9695312   host,patrick,neil  \n",
       "4                    None              None   host,patrick,neil  \n",
       "...                   ...               ...                 ...  \n",
       "75341  536719881299697665        2379738985  music,beijing,wave  \n",
       "75342  536709870976659456        1067944501  music,beijing,wave  \n",
       "75343  536719881299697665        2379738985  music,beijing,wave  \n",
       "75344  536717467662700544         371539066  music,beijing,wave  \n",
       "75345                None              None  music,beijing,wave  \n",
       "\n",
       "[75346 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_cred_avg(cred_list):\n",
    "  cred_list = cred_list[1:-1]\n",
    "  creds = map(int, re.findall(r'-?\\d+', cred_list))\n",
    "  #print((list(creds)))\n",
    "  return sum(creds)/30\n",
    "print(data_df.loc[0])\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "2_aWV9HapXIz",
    "outputId": "52633225-a6c2-493b-a0a9-464561155cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.000000\n",
      "1       1.333333\n",
      "2       2.000000\n",
      "3       1.833333\n",
      "4       1.900000\n",
      "          ...   \n",
      "1374    1.133333\n",
      "1375    1.833333\n",
      "1376    1.533333\n",
      "1377    1.966667\n",
      "1378    1.866667\n",
      "Name: avg_cred, Length: 1379, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "cred_df['avg_cred'] = cred_df['Cred_Ratings'].apply(get_cred_avg)\n",
    "print(cred_df['avg_cred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "XUz4aun6p50t",
    "outputId": "d37d8cc5-8033-4e01-f0cf-29152d3078c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                         RT @thefashionisto: Neil Patrick Harris Tapped...\n",
      "id                                                          522749079617806336\n",
      "author_id                                                           2253554083\n",
      "created_at                                                 2014-10-16 14:01:07\n",
      "in_reply_to_status_id_str                                                  NaN\n",
      "in_reply_to_user_id_str                                                    NaN\n",
      "in_reply_to_screen_name                                                    NaN\n",
      "quoted_status_id_str                                                      None\n",
      "retweet_id                                                  522748889204809728\n",
      "retweet_author_id                                                     15325349\n",
      "topic                                                        host,patrick,neil\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(cred_df.loc[0])\n",
    "print(data_df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cs4gz60rMJy"
   },
   "outputs": [],
   "source": [
    "def get_topic(topic_key):\n",
    "  topic = topic_key.split(\"-\", 1)\n",
    "  return topic[0].replace(\"_\", \",\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "5QxkLedesA-T",
    "outputId": "26d68e56-b016-4af6-f0bd-a7cf6e263db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     topic,key\n",
      "1        everything,royals,rain\n",
      "2             host,patrick,neil\n",
      "3            royals,game,series\n",
      "4               giants,game,win\n",
      "                 ...           \n",
      "1374          god,golden,awards\n",
      "1375       pope,francis,welcome\n",
      "1376       golden,globes,taylor\n",
      "1377          streak,hawks,game\n",
      "1378    liverpool,league,lovren\n",
      "Name: topic, Length: 1379, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cred_df['topic'] = cred_df['topic_key'].apply(get_topic)\n",
    "print(cred_df['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "4t0o3s6zsJpi",
    "outputId": "322c0fa9-ee71-4262-eb19-3e40c4b29a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text                  id  \\\n",
      "0      RT @thefashionisto: Neil Patrick Harris Tapped...  522749079617806336   \n",
      "1      Neil Patrick Harris to host Oscars: Neil Patri...  522744418126675972   \n",
      "2      The 2015 @TheAcademy host @ActuallyNPH on @Mat...  522744773879558145   \n",
      "3      RT @billboard: Oscar Host Neil Patrick Harris:...  522742777411407872   \n",
      "4      Neil Patrick Harris set to host Oscars 2015 ht...  522741609969225728   \n",
      "...                                                  ...                 ...   \n",
      "75341  RT @HelloImIvan: odell beckham jr could catch ...  536720198402060288   \n",
      "75342  RT @ericsports: Zoomed in picture of Odell Bec...  536720169641705473   \n",
      "75343  RT @HelloImIvan: odell beckham jr could catch ...  536720146581430273   \n",
      "75344  RT @ShannonSharpe: I've been to 4 county fairs...  536720156417069057   \n",
      "75345  â€œ@nfl: Odell Beckham just broke the internet...  536720064452780032   \n",
      "\n",
      "        author_id           created_at  in_reply_to_status_id_str  \\\n",
      "0      2253554083  2014-10-16 14:01:07                        NaN   \n",
      "1       377665274  2014-10-16 13:42:36                        NaN   \n",
      "2        36044387  2014-10-16 13:44:00                        NaN   \n",
      "3       179568487  2014-10-16 13:36:04                        NaN   \n",
      "4      2773297820  2014-10-16 13:31:26                        NaN   \n",
      "...           ...                  ...                        ...   \n",
      "75341   153282458  2014-11-24 03:17:21                        NaN   \n",
      "75342   314385813  2014-11-24 03:17:14                        NaN   \n",
      "75343   297042888  2014-11-24 03:17:09                        NaN   \n",
      "75344    99869923  2014-11-24 03:17:11                        NaN   \n",
      "75345   548837960  2014-11-24 03:16:49               5.367042e+17   \n",
      "\n",
      "       in_reply_to_user_id_str in_reply_to_screen_name quoted_status_id_str  \\\n",
      "0                          NaN                     NaN                 None   \n",
      "1                          NaN                     NaN                 None   \n",
      "2                          NaN                     NaN                 None   \n",
      "3                          NaN                     NaN                 None   \n",
      "4                          NaN                     NaN                 None   \n",
      "...                        ...                     ...                  ...   \n",
      "75341                      NaN                     NaN                 None   \n",
      "75342                      NaN                     NaN                 None   \n",
      "75343                      NaN                     NaN                 None   \n",
      "75344                      NaN                     NaN                 None   \n",
      "75345               19426551.0                     NFL                 None   \n",
      "\n",
      "               retweet_id retweet_author_id               topic  \\\n",
      "0      522748889204809728          15325349   host,patrick,neil   \n",
      "1                    None              None   host,patrick,neil   \n",
      "2                    None              None   host,patrick,neil   \n",
      "3      522716566610530304           9695312   host,patrick,neil   \n",
      "4                    None              None   host,patrick,neil   \n",
      "...                   ...               ...                 ...   \n",
      "75341  536719881299697665        2379738985  music,beijing,wave   \n",
      "75342  536709870976659456        1067944501  music,beijing,wave   \n",
      "75343  536719881299697665        2379738985  music,beijing,wave   \n",
      "75344  536717467662700544         371539066  music,beijing,wave   \n",
      "75345                None              None  music,beijing,wave   \n",
      "\n",
      "                                               topic_key  \\\n",
      "0      host_patrick_neil-20141015_161647-20141015_172214   \n",
      "1      host_patrick_neil-20141015_161647-20141015_172214   \n",
      "2      host_patrick_neil-20141015_161647-20141015_172214   \n",
      "3      host_patrick_neil-20141015_161647-20141015_172214   \n",
      "4      host_patrick_neil-20141015_161647-20141015_172214   \n",
      "...                                                  ...   \n",
      "75341  music_beijing_wave-20141025_094413-20141025_10...   \n",
      "75342  music_beijing_wave-20141025_094413-20141025_10...   \n",
      "75343  music_beijing_wave-20141025_094413-20141025_10...   \n",
      "75344  music_beijing_wave-20141025_094413-20141025_10...   \n",
      "75345  music_beijing_wave-20141025_094413-20141025_10...   \n",
      "\n",
      "                           topic_terms  \\\n",
      "0       [u'host', u'patrick', u'neil']   \n",
      "1       [u'host', u'patrick', u'neil']   \n",
      "2       [u'host', u'patrick', u'neil']   \n",
      "3       [u'host', u'patrick', u'neil']   \n",
      "4       [u'host', u'patrick', u'neil']   \n",
      "...                                ...   \n",
      "75341  [u'music', u'beijing', u'wave']   \n",
      "75342  [u'music', u'beijing', u'wave']   \n",
      "75343  [u'music', u'beijing', u'wave']   \n",
      "75344  [u'music', u'beijing', u'wave']   \n",
      "75345  [u'music', u'beijing', u'wave']   \n",
      "\n",
      "                                            Cred_Ratings  \\\n",
      "0      ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
      "1      ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
      "2      ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
      "3      ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
      "4      ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
      "...                                                  ...   \n",
      "75341  ['1', '2', '2', '2', '1', '2', '2', '2', '0', ...   \n",
      "75342  ['1', '2', '2', '2', '1', '2', '2', '2', '0', ...   \n",
      "75343  ['1', '2', '2', '2', '1', '2', '2', '2', '0', ...   \n",
      "75344  ['1', '2', '2', '2', '1', '2', '2', '2', '0', ...   \n",
      "75345  ['1', '2', '2', '2', '1', '2', '2', '2', '0', ...   \n",
      "\n",
      "                                                 Reasons  avg_cred  \n",
      "0      ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
      "1      ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
      "2      ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
      "3      ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
      "4      ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
      "...                                                  ...       ...  \n",
      "75341  ['Seems to be about real things', 'several ent...  1.466667  \n",
      "75342  ['Seems to be about real things', 'several ent...  1.466667  \n",
      "75343  ['Seems to be about real things', 'several ent...  1.466667  \n",
      "75344  ['Seems to be about real things', 'several ent...  1.466667  \n",
      "75345  ['Seems to be about real things', 'several ent...  1.466667  \n",
      "\n",
      "[75346 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cred_df['topic']=cred_df['topic'].astype(str)\n",
    "data_df['topic']=data_df['topic'].astype(str)\n",
    "cred_topic_df= pd.concat([cred_df['topic'],cred_df['avg_cred']],axis=1) \n",
    "#df_merge = data_df.merge(cred_df, on='topic', how='left')\n",
    "df_merge = data_df.merge(cred_df.drop_duplicates(subset=['topic']))\n",
    "print(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1joQydttHU-"
   },
   "outputs": [],
   "source": [
    "df_merge = df_merge.sort_values(by=['avg_cred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zwWscdKD0p-I"
   },
   "outputs": [],
   "source": [
    "head  = df_merge.sort_values(by=['avg_cred']).head(10000)\n",
    "tail = df_merge.sort_values(by=['avg_cred']).tail(10000)\n",
    "\n",
    "result = pd.concat([head,tail])\n",
    "result.to_csv('train_set_with_avg_cred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "yN-MlAqb0311",
    "outputId": "0d044487-044f-4ec1-c18d-6c5331eb48b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                    text                  id  \\\n",
       "3616  RT @EWNsport: The West Indies Cricket Board ha...  523158541298069504   \n",
       "4000  RT @LastWeekTonight: President Obama has named...  523493185059028993   \n",
       "3999  RT @WashTimes: White House: #Ebola 'czar' Ron ...  523504739900989440   \n",
       "3998  Obama's Newly-Appointed 'Ebola Czar' Happens t...  523508962847506432   \n",
       "3997  The Serious Problem With Obama's Choice Of Ron...  523515774636683264   \n",
       "...                                                 ...                 ...   \n",
       "3     RT @billboard: Oscar Host Neil Patrick Harris:...  522742777411407872   \n",
       "2     The 2015 @TheAcademy host @ActuallyNPH on @Mat...  522744773879558145   \n",
       "1     Neil Patrick Harris to host Oscars: Neil Patri...  522744418126675972   \n",
       "118   RT @usweekly: It's on! Neil Patrick Harris wil...  522529994157731840   \n",
       "0     RT @thefashionisto: Neil Patrick Harris Tapped...  522749079617806336   \n",
       "\n",
       "       author_id           created_at  in_reply_to_status_id_str  \\\n",
       "3616    32824308  2014-10-17 17:08:10                        NaN   \n",
       "4000   788796614  2014-10-18 15:17:56                        NaN   \n",
       "3999    16058324  2014-10-18 16:03:50                        NaN   \n",
       "3998   731710176  2014-10-18 16:20:37                        NaN   \n",
       "3997  2506199450  2014-10-18 16:47:41                        NaN   \n",
       "...          ...                  ...                        ...   \n",
       "3      179568487  2014-10-16 13:36:04                        NaN   \n",
       "2       36044387  2014-10-16 13:44:00                        NaN   \n",
       "1      377665274  2014-10-16 13:42:36                        NaN   \n",
       "118    174928232  2014-10-15 23:30:33                        NaN   \n",
       "0     2253554083  2014-10-16 14:01:07                        NaN   \n",
       "\n",
       "      in_reply_to_user_id_str in_reply_to_screen_name quoted_status_id_str  \\\n",
       "3616                      NaN                     NaN                 None   \n",
       "4000                      NaN                     NaN                 None   \n",
       "3999                      NaN                     NaN                 None   \n",
       "3998                      NaN                     NaN                 None   \n",
       "3997                      NaN                     NaN                 None   \n",
       "...                       ...                     ...                  ...   \n",
       "3                         NaN                     NaN                 None   \n",
       "2                         NaN                     NaN                 None   \n",
       "1                         NaN                     NaN                 None   \n",
       "118                       NaN                     NaN                 None   \n",
       "0                         NaN                     NaN                 None   \n",
       "\n",
       "              retweet_id retweet_author_id               topic  \\\n",
       "3616  523137164859965440         242308246  ebola,obama,#ebola   \n",
       "4000  523476990087933952        2317351705  ebola,obama,#ebola   \n",
       "3999  523489001291128832          14662354  ebola,obama,#ebola   \n",
       "3998                None              None  ebola,obama,#ebola   \n",
       "3997                None              None  ebola,obama,#ebola   \n",
       "...                  ...               ...                 ...   \n",
       "3     522716566610530304           9695312   host,patrick,neil   \n",
       "2                   None              None   host,patrick,neil   \n",
       "1                   None              None   host,patrick,neil   \n",
       "118   522505082911596544          20012204   host,patrick,neil   \n",
       "0     522748889204809728          15325349   host,patrick,neil   \n",
       "\n",
       "                                              topic_key  \\\n",
       "3616  ebola_obama_#ebola-20141016_181953-20141016_19...   \n",
       "4000  ebola_obama_#ebola-20141016_181953-20141016_19...   \n",
       "3999  ebola_obama_#ebola-20141016_181953-20141016_19...   \n",
       "3998  ebola_obama_#ebola-20141016_181953-20141016_19...   \n",
       "3997  ebola_obama_#ebola-20141016_181953-20141016_19...   \n",
       "...                                                 ...   \n",
       "3     host_patrick_neil-20141015_161647-20141015_172214   \n",
       "2     host_patrick_neil-20141015_161647-20141015_172214   \n",
       "1     host_patrick_neil-20141015_161647-20141015_172214   \n",
       "118   host_patrick_neil-20141015_161647-20141015_172214   \n",
       "0     host_patrick_neil-20141015_161647-20141015_172214   \n",
       "\n",
       "                          topic_terms  \\\n",
       "3616  [u'ebola', u'obama', u'#ebola']   \n",
       "4000  [u'ebola', u'obama', u'#ebola']   \n",
       "3999  [u'ebola', u'obama', u'#ebola']   \n",
       "3998  [u'ebola', u'obama', u'#ebola']   \n",
       "3997  [u'ebola', u'obama', u'#ebola']   \n",
       "...                               ...   \n",
       "3      [u'host', u'patrick', u'neil']   \n",
       "2      [u'host', u'patrick', u'neil']   \n",
       "1      [u'host', u'patrick', u'neil']   \n",
       "118    [u'host', u'patrick', u'neil']   \n",
       "0      [u'host', u'patrick', u'neil']   \n",
       "\n",
       "                                           Cred_Ratings  \\\n",
       "3616  ['1', '0', '2', '2', '2', '-2', '-2', '2', '2'...   \n",
       "4000  ['1', '0', '2', '2', '2', '-2', '-2', '2', '2'...   \n",
       "3999  ['1', '0', '2', '2', '2', '-2', '-2', '2', '2'...   \n",
       "3998  ['1', '0', '2', '2', '2', '-2', '-2', '2', '2'...   \n",
       "3997  ['1', '0', '2', '2', '2', '-2', '-2', '2', '2'...   \n",
       "...                                                 ...   \n",
       "3     ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
       "2     ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
       "1     ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
       "118   ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
       "0     ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...   \n",
       "\n",
       "                                                Reasons  avg_cred  \n",
       "3616  ['obama wants to intensify action against ebol...  0.666667  \n",
       "4000  ['obama wants to intensify action against ebol...  0.666667  \n",
       "3999  ['obama wants to intensify action against ebol...  0.666667  \n",
       "3998  ['obama wants to intensify action against ebol...  0.666667  \n",
       "3997  ['obama wants to intensify action against ebol...  0.666667  \n",
       "...                                                 ...       ...  \n",
       "3     ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
       "2     ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
       "1     ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
       "118   ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
       "0     ['Neil Patrick Harris will host the 2015 Oscar...  2.000000  \n",
       "\n",
       "[30000 rows x 16 columns]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "PMKtjT4AeU3S",
    "outputId": "29d1b72d-5143-4a86-8a9b-04f9b1ccb24d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ycm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ycm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ycm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import time\n",
    "import statistics as st\n",
    "\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np \n",
    "import nltk \n",
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster import KMeansClusterer    \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "full_dataset= result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "luXqlc9fZyqX"
   },
   "outputs": [],
   "source": [
    "\n",
    "#########################################################################################################\n",
    "#The Elbow Method\n",
    "def calculate_WSS(points, kmax): \n",
    "  sse = []\n",
    "  for k in range(1, kmax+1):\n",
    "    kmeans = KMeans(n_clusters = k).fit(points)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    pred_clusters = kmeans.predict(points)\n",
    "    curr_sse = 0\n",
    "    \n",
    "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "    for i in range(len(points)):\n",
    "      curr_center = centroids[pred_clusters[i]]\n",
    "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "      \n",
    "    sse.append(curr_sse)\n",
    "    plt.plot(sse)\n",
    "  return sse\n",
    "\n",
    "#########################################################################################################\n",
    "#vectorization of sentence \n",
    "def sent_vectorizer(sent):\n",
    "    sent_vec =[]\n",
    "    numw = 0\n",
    "    for w in sent:\n",
    "        try:\n",
    "            sent_vec.append(model[w])\n",
    "        except:\n",
    "            pass\n",
    "     \n",
    "    return sent_vec\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "#lemmatization od sentence \n",
    "def lemmatization(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "#########################################################################################################\n",
    "#stemming word  \n",
    "def stemmeriation(text):\n",
    "    return [ps.stem(w) for w in text]\n",
    "    \n",
    "#########################################################################################################\n",
    "#get the mean of the vector of a sentence \n",
    "def avg_vect(sent_vec):\n",
    "    sum_of_vec = 0\n",
    "    for word in sent_vec:\n",
    "      sum_of_vec+= np.mean(word)\n",
    "    if(len(sent_vec)>0):\n",
    "      return (sum_of_vec/len(sent_vec))  \n",
    "    return 0\n",
    "#########################################################################################################\n",
    "#print each sentence \n",
    "def sent_print(sent):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "2ABZa5t_Z1qX",
    "outputId": "2b9e5bc1-2721-4a30-fe94-f7e574114ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                         RT @thefashionisto: Neil Patrick Harris Tapped...\n",
      "id                                                          522749079617806336\n",
      "author_id                                                           2253554083\n",
      "created_at                                                 2014-10-16 14:01:07\n",
      "in_reply_to_status_id_str                                                  NaN\n",
      "in_reply_to_user_id_str                                                    NaN\n",
      "in_reply_to_screen_name                                                    NaN\n",
      "quoted_status_id_str                                                      None\n",
      "retweet_id                                                  522748889204809728\n",
      "retweet_author_id                                                     15325349\n",
      "topic                                                        host,patrick,neil\n",
      "topic_key                    host_patrick_neil-20141015_161647-20141015_172214\n",
      "topic_terms                                     [u'host', u'patrick', u'neil']\n",
      "Cred_Ratings                 ['2', '2', '2', '2', '2', '2', '2', '2', '2', ...\n",
      "Reasons                      ['Neil Patrick Harris will host the 2015 Oscar...\n",
      "avg_cred                                                                     2\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(full_dataset.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "ow6IiItKZ25X",
    "outputId": "3597a029-f982-43d2-e9ea-30a2640c8802"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\src\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\src\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#text pre-processing\n",
    "not_words = ['!','@','?',',','.',\"'\", ')',';','(', '/',':','-','&','*','\"','\\'','\\\"','...','..','=]','=[','rt']\n",
    "\n",
    "tweet_dataset = full_dataset[['id','text','avg_cred']]\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "for sent in tweet_dataset['text']:\n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(str(sent).lower()))\n",
    "tweet_dataset['tokenized_text'] = tweet_tokens\n",
    "\n",
    "tweet_dataset['tokenized_text'] = tweet_dataset['tokenized_text'].apply(lambda x: [item for item in x if item not in not_words])\n",
    "\n",
    "for sentence in tweet_dataset['tokenized_text']: \n",
    "    num_to_del = 0\n",
    "    for word in sentence:\n",
    "        if '@' in word: \n",
    "            num_to_del +=1\n",
    "    for i in range(0,num_to_del):\n",
    "        for word in sentence:\n",
    "            if '@' in word: \n",
    "                sentence.remove(word)  \n",
    "                \n",
    "for sentence in tweet_dataset['tokenized_text']: \n",
    "    num_to_del = 0\n",
    "    for word in sentence:\n",
    "        if 'http' in word: \n",
    "            num_to_del +=1\n",
    "    for i in range(0,num_to_del):\n",
    "        for word in sentence:\n",
    "            if 'http' in word: \n",
    "                sentence.remove(word)  \n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "1SRrC1IBZ4WD",
    "outputId": "02a31749-6095-428b-e2c8-b9bda8704e17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\src\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\src\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3616    [west, indi, cricket, board, deni, report, tea...\n",
      "4000    [presid, obama, name, ebola, czar, might, wors...\n",
      "3999    [white, hous, #ebola, czar, ron, klain, report...\n",
      "3998    [obama', newly-appoint, ebola, czar, happen, l...\n",
      "3997    [seriou, problem, obama', choic, ron, klain, e...\n",
      "                              ...                        \n",
      "3       [oscar, host, neil, patrick, harri, watch, 7, ...\n",
      "2                        [2015, host, talk, he', tomm, >]\n",
      "1       [neil, patrick, harri, host, oscar, neil, patr...\n",
      "118     [neil, patrick, harri, host, 2015, oscar, febr...\n",
      "0                [neil, patrick, harri, tap, host, oscar]\n",
      "Name: lemmatized_text, Length: 30000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\src\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#remove stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "tweet_dataset['text_without_stop_word'] = tweet_dataset['tokenized_text'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "#stem\n",
    "ps = PorterStemmer() \n",
    "tweet_dataset['stemmed_text'] = tweet_dataset['text_without_stop_word'].apply(stemmeriation)\n",
    "\n",
    "#lemma\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "tweet_dataset['lemmatized_text'] = tweet_dataset['stemmed_text'].apply(lemmatization)\n",
    "\n",
    "print(tweet_dataset['lemmatized_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "W8Go7UDFZ6Ca",
    "outputId": "45a1eea5-b703-4292-b794-4e802037306d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 3.6% 59.6/1662.8MB downloaded"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-902cc43e311d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word2vec-google-news-300\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#model = Word2Vec(tweet_dataset['lemmatized_text'], min_count=1,size= 32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         \u001b[0m_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36m_download\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{fname}.gz\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mdst_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_calculate_md5_checksum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_get_checksum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1069\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\src\\Anaconda\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    927\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#build word2vec model and run on each sentences \n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "path = api.load(\"word2vec-google-news-300\", return_path=True)\n",
    "print(path)\n",
    "#model = Word2Vec(tweet_dataset['lemmatized_text'], min_count=1,size= 32)\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "#tweet_dataset['vector_text'] = np.array([model[word] for word in (model.wv.vocab)])\n",
    "tweet_dataset['vector_text'] = tweet_dataset['lemmatized_text'].apply(sent_vectorizer)\n",
    "\n",
    "print((tweet_dataset['vector_text'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ss7WKoJVZ9Le"
   },
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#get the avg vector of all vectors in a sentence \n",
    "def avg_vector(vect_list):\n",
    "  return np.mean(vect_list, axis=0)\n",
    "\n",
    "def max_vector(vect_list):\n",
    "  return np.maximum(vect_list)\n",
    "\n",
    "def min_vector(vect_list):\n",
    "  tmp=[]\n",
    "  for v in vect_list:\n",
    "    tmp.append(float(v))\n",
    "  return min(tmp)\n",
    "#return the len of the sentence \n",
    "def get_text_len(text):\n",
    "  return len(text)\n",
    "\n",
    "\n",
    "tweet_dataset['avg_vector_text'] = tweet_dataset['vector_text'].apply(avg_vector)\n",
    "#tweet_dataset['max_vector_text'] = tweet_dataset['vector_text'].apply(max_vector)\n",
    "#tweet_dataset['min_vector_text'] = tweet_dataset['vector_text'].apply(min_vector)\n",
    "\n",
    "tweet_dataset['len_text'] = tweet_dataset['vector_text'].apply(get_text_len)\n",
    "\n",
    "############################################################################################\n",
    "#replace all nan value of sentence into zero vector\n",
    "def get_list(vect_list):\n",
    "  re_list = []\n",
    "  #print(vect_list)\n",
    "  if not np.isnan(vect_list).any():   \n",
    "    return vect_list\n",
    "  else:\n",
    "    return ([0] * 32)\n",
    "############################################################################################\n",
    "#iterative to call the \n",
    "def loop_rolist(vect_list):\n",
    "  return vect_list.tolist()\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "tweet_dataset['avg_vector_text'] = tweet_dataset['avg_vector_text'].apply(get_list)\n",
    "#tweet_dataset['max_vector_text'] = tweet_dataset['max_vector_text'].apply(get_list)\n",
    "#tweet_dataset['min_vector_text'] = tweet_dataset['min_vector_text'].apply(get_list)\n",
    "#tweet_dataset['len_text'] = tweet_dataset['len_text'].apply(get_list)\n",
    "print(tweet_dataset['avg_vector_text'])\n",
    "wws_dataset = tweet_dataset.sample(n=10000)\n",
    "vec_data = pd.DataFrame(tweet_dataset['avg_vector_text'].tolist(),tweet_dataset['avg_cred'].tolist() )\n",
    "#vec_data = pd.DataFrame(tweet_dataset['avg_vector_text'].tolist())\n",
    "\n",
    "\n",
    "\n",
    "print(vec_data)\n",
    "\n",
    "#vec_data: pandas to cluster, contain the 32 col of average vector and one col contain the length of a the sentence \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9udPFgEZ_ex"
   },
   "outputs": [],
   "source": [
    "#normalization\n",
    "from sklearn import preprocessing\n",
    "vec_data[np.isnan(vec_data)] = 0\n",
    "x = vec_data \n",
    "min_max_scaler = preprocessing.Normalizer()\n",
    "normalizsed_dataset_array = min_max_scaler.fit_transform(x)\n",
    "nor_df = pd.DataFrame(normalizsed_dataset_array)\n",
    "plt.plot(nor_df[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iS5ZkXlHaBO7"
   },
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "#we have not enough computation power to run, even colab have memory error \n",
    "start_time = time.time()\n",
    "clustering = DBSCAN(eps=0.1).fit(normalizsed_dataset_array)\n",
    "tweet_dataset['part1_gp'] =clustering.labels_ \n",
    "full_dataset['part1_gp'] = clustering.labels_ \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(full_dataset)\n",
    "print(tweet_dataset.gp.max())\n",
    "full_dataset.to_csv('DBSCAN_part1_clsutering.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIIid-vLaFHc"
   },
   "outputs": [],
   "source": [
    "#make gp as a col and output it as a csv file\n",
    "no_gp = tweet_dataset.gp.max() + 1\n",
    "full_dataset['gp'] = labels \n",
    "full_dataset.to_csv('COMP4641_text_clustering_with_cred_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46N9SpufaiIa"
   },
   "outputs": [],
   "source": [
    "full_dataset = full_dataset[full_dataset.gp != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jztv4ns2akN_"
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#reomve the time zone from str \n",
    "def removePDT (str):\n",
    "  return str.remove('PDT')\n",
    "\n",
    "###############################################################\n",
    "#turn datatime into second\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day\n",
    "###############################################################\n",
    "#(t1-t2)/24\n",
    "#t1 = the time of each tweet\n",
    "#t2 = the centriod time of each group\n",
    "def cal_time_different(t1,t2):\n",
    "  try:\n",
    "    different = (t1-t2)\n",
    "    different_min = different.total_seconds()/60\n",
    "\n",
    "    \n",
    "    if abs(different_min) >= (24*60):\n",
    "      return 0\n",
    "    else:\n",
    "      return (different_min/(24*60))\n",
    "  except:\n",
    "    return 0\n",
    "###############################################################\n",
    "def get_time_different_list(df,mean_list):\n",
    "  diff_list=[]\n",
    "  for i,row in df.iterrows():\n",
    "    t2 = mean_list[row['gp']]\n",
    "    t1 = row['time']\n",
    "    diff_list.append(cal_time_different(t1,t2))\n",
    "\n",
    "  return diff_list\n",
    "  \n",
    "###############################################################\n",
    "def cal_time_average(df,no_cluster):\n",
    "    mean_list = []\n",
    "    print(df)\n",
    "    for i in range(no_cluster):\n",
    "        df1 = df.loc[df['gp'] == i]\n",
    "        if len(df1) == 0:\n",
    "          timestamp = 0\n",
    "        else:\n",
    "          sum = 0\n",
    "          for time in df1['time_stamp']:\n",
    "            sum +=time\n",
    "          timestamp = sum/len(df1) \n",
    "        dt_object = datetime.fromtimestamp(timestamp)   \n",
    "        mean_list.append(dt_object)\n",
    "        print(\"group\",i, dt_object)\n",
    "    return mean_list\n",
    "###############################################################\n",
    "def cal_time_mode(df,no_cluster):\n",
    "    mean_list = []\n",
    "    print(df)\n",
    "    for i in range(no_cluster):\n",
    "        df1 = df.loc[df['gp'] == i]\n",
    "        sum = 0\n",
    "        if len(df1) !=0:\n",
    "          for time in df1['time_stamp']:\n",
    "            sum +=time\n",
    "          timestamp = sum/len(df1)\n",
    "        else:\n",
    "          timestamp = 0\n",
    "        dt_object = datetime.fromtimestamp(timestamp)   \n",
    "        mean_list.append(dt_object)\n",
    "        print(\"group\",i, dt_object)\n",
    "    return mean_list\n",
    "\n",
    "###############################################################\n",
    "def to_timestamp(dt):\n",
    "    dt = dt.timestamp()\n",
    "    return dt\n",
    "\n",
    "###############################################################\n",
    "#turn str to datetime\n",
    "def to_datetime(date_str):\n",
    "  date_str = date_str.replace(\"PDT\", \"\")\n",
    "  return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "###############################################################\n",
    "#df.drop([0, 1])\n",
    "def try_to_datetime(df):\n",
    "  error_time_index = []\n",
    "  for index, row in df.iterrows():\n",
    "    try:\n",
    "      to_datetime(row['created_at'])\n",
    "    except:\n",
    "      error_time_index.append(index)\n",
    "      continue\n",
    "  print(error_time_index)\n",
    "  df = df.drop(error_time_index)\n",
    "  df['time'] = df['created_at'].apply(to_datetime)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jpy6i9Ozak3D"
   },
   "outputs": [],
   "source": [
    "tweet_dataset = full_dataset \n",
    "#tweet_dataset['time'] = full_dataset['created_at'].apply(to_datetime)\n",
    "\n",
    "tweet_dataset = try_to_datetime(tweet_dataset)\n",
    "tweet_dataset['time_stamp'] = tweet_dataset['time'].apply(to_timestamp)\n",
    "print(tweet_dataset.loc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iB4ssllVamtL"
   },
   "outputs": [],
   "source": [
    "#find mean of each cluster\n",
    "mean_list = cal_time_average(tweet_dataset,6)\n",
    "print(mean_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwG9_MiGapKI"
   },
   "outputs": [],
   "source": [
    "#print(timestamp(datetime_object[1])-timestamp(datetime_object[303]))\n",
    "#print((datetime_object[303]))\n",
    "tweet_dataset['time_different'] = get_time_different_list(tweet_dataset,mean_list)\n",
    "\n",
    "print(tweet_dataset['time_different'].max())\n",
    "#print(cal_time_different(datetime_object[50],datetime_object[1]))\n",
    "#print(cal_time_different(datetime_object[1],datetime_object[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4SmVz6aaqjW"
   },
   "outputs": [],
   "source": [
    "print(tweet_dataset.loc[0])\n",
    "\n",
    "drop_dataset = tweet_dataset.drop(['topic_terms','Reasons','time','text','id','author_id','created_at','in_reply_to_status_id_str','in_reply_to_user_id_str','in_reply_to_screen_name','quoted_status_id_str','retweet_id','retweet_author_id','topic'\n",
    "                                  ,'topic_key','Cred_Ratings']\n",
    ",axis=1)\n",
    "print(drop_dataset.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1JC_446arsT"
   },
   "outputs": [],
   "source": [
    "#normalization\n",
    "x = drop_dataset.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "normalizsed_dataset_array = min_max_scaler.fit_transform(x)\n",
    "nor_df = pd.DataFrame(normalizsed_dataset_array)\n",
    "\n",
    "#print(tweet_dataset)\n",
    "print(nor_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZfkG4FoatJU"
   },
   "outputs": [],
   "source": [
    "def WWS2(data):\n",
    "  print('hi')\n",
    "  Sum_of_squared_distances = []\n",
    "  K = range(1,20)\n",
    "  for k in K:\n",
    "      km = KMeans(n_clusters=k)\n",
    "      km = km.fit(data)\n",
    "      Sum_of_squared_distances.append(km.inertia_)\n",
    "  plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "  plt.xlabel('k')\n",
    "  plt.ylabel('Sum_of_squared_distances')\n",
    "  plt.title('Elbow Method For Optimal k')\n",
    "  plt.show()\n",
    "\n",
    "WWS2(nor_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jsJ8F4Aauds"
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "print(\"START K-MEAN\")\n",
    "kmeans = cluster.KMeans(n_clusters=7)\n",
    "start_time = time.time()\n",
    "kmeans.fit(nor_df)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print (\"Cluster id labels for inputted data\")\n",
    "print (labels)\n",
    "\n",
    " \n",
    "#silhouette_score = metrics.silhouette_score(nor_df, labels, metric='euclidean')\n",
    " \n",
    "#print (\"Silhouette_score: \")\n",
    "#print (silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uu_e1pB1awM-"
   },
   "outputs": [],
   "source": [
    "tweet_dataset['part1_gp'] = labels\n",
    "tweet_dataset.to_csv('k_mean_PART1_CREDBANK_with_cred.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07N3ojBJax8u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "match_cred.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
